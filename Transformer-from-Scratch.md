# 从零开始构建 Transformer 模型

[布兰登·罗勒](https://brandonrohrer.com/blog.html)

我拖延了好几年才开始深入研究变形金刚。最终，对它们运作原理一无所知的焦虑感让我难以忍受。现在，我终于开始深入研究了。

[Transformer模型在2017年的一篇论文](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)中被提出，作为一种序列转换工具——将一个符号序列转换为另一个符号序列。最常见的例子是翻译，例如英语到德语的翻译。它也被改进用于序列补全——给定一个起始提示，以相同的思路和风格继续进行。Transformer模型迅速成为自然语言处理领域研究和产品开发中不可或缺的工具。

开始之前，先提醒一下。我们会大量讲解矩阵乘法，还会涉及反向传播（模型训练算法），但你不需要事先了解这些内容。我们会逐步讲解所需的概念，并加以解释。

这趟旅程并不短，但我希望你不会后悔来到这里。

- [独热编码](#one_hot)
- [点积](#dot_product)
- [矩阵乘法](#matrix_multiplication)
- [矩阵乘法作为查表](#table_lookup)
- [一阶序列模型](#markov_chain)
- [二阶序列模型](#second_order)
- [带跳跃的二阶序列模型](#second_order_skips)
- [掩码](#masking)
- [休息站和出口匝道](#rest_stop)
- [注意力如同矩阵乘法](#attention)
- [二阶序列模型作为矩阵乘法](#second_order_matrix_mult)
- [序列完成](#sequence_completion)
- [嵌入](#embeddings)
- [位置编码](#positional_encoding)
- [反向嵌入](#deembeddings)
- [Softmax](#softmax)
- [多头注意力](#multihead)
- [单头注意力再探](#attention_revisited)
- [跳过连接](#skip_connections)
- [多层](#layer_stack)
- [解码器栈](#decoder)
- [编码器堆栈](#encoder)
- [交叉注意力](#cross_attention)
- [分词](#tokenizing)
- [字节对编码](#bpe)
- [音频输入](#audio_input)
- [资源和版权](#resources)

### 独热编码

起初是文字。数不胜数的文字。我们的第一步是将所有文字转换成数字，以便进行数学运算。

想象一下，我们的目标是创造一台能够响应我们语音指令的计算机。我们的任务是构建一个转换器，将一系列声音转换（或**转导**）成一系列词语。

首先，我们需要选择**词汇表**，也就是我们在每个序列中要使用的符号集合。在本例中，我们将使用两组不同的符号，一组用于输入序列，代表语音；另一组用于输出序列，代表单词。

现在，我们假设我们处理的是英语。英语中有数万个单词，再加上几千个计算机专业术语，词汇量就接近十万了。一种将单词转换为数字的方法是，从 1 开始计数，并为每个单词分配一个数字。这样，一系列单词就可以表示为一个数字列表。

例如，考虑一种词汇量只有三个词的微型语言：*files*、*find*和*my*。每个词都可以用一个数字替换，例如*files* = 1、*find* = 2 和*my = 3。那么，由词序列 [* *find* , *my* , *files* ]组成的句子“Find my files”就可以表示为数字序列 [2, 3, 1]。

这是一种完全有效的符号转数字方法，但实际上还有一种更易于计算机处理的格式：**独热编码**。在独热编码中，一个符号由一个几乎全为零的数组表示，数组长度与词汇表长度相同，只有一个元素的值为 1。数组中的每个元素对应一个独立的符号。

另一种理解独热编码的方式是，每个单词仍然会被分配一个数字，但现在这个数字是数组的索引。以下是上面示例的独热编码表示。

![独热编码词汇表](image/transformer/one_hot_vocabulary.png)

因此，“查找我的文件”这句话变成了一系列一维数组，当你把它们压缩在一起后，它们看起来就像一个二维数组。

![独热编码句子](image/transformer/one_hot_sentence.png)

请注意，我将交替使用“一维数组”和“**向量**”这两个术语。同样，“二维数组”和“**矩阵**”也是如此。

### 点积

独热编码的一个真正实用之处在于它允许我们计算[点积](https://en.wikipedia.org/wiki/Dot_product)。点积还有其他一些令人望而生畏的名称，例如内积和标量积。要计算两个向量的点积，只需将它们的对应元素相乘，然后将结果相加即可。

![点积图](image/transformer/dot_product.png)

点积在处理独热词表示时特别有用。任何独热向量与其自身的点积都为 1。

![匹配向量的点积](image/transformer/match.png)

任意独热向量与任意其他独热向量的点积均为零。

![不匹配向量的点积](image/transformer/non_match.png)

前两个例子展示了如何使用点积来衡量相似度。再举一个例子，考虑一个表示不同权重词语组合的值向量。我们可以将一个独热编码的词语与该向量进行点积比较，以显示该词语的表示强度。

![点积表示两个向量之间的相似度。](image/transformer/similarity.png)

### 矩阵乘法

点积是矩阵乘法的基本组成部分，矩阵乘法是一种将两个二维数组组合起来的特殊方法。我们把第一个矩阵称为*A*，第二个矩阵称为*B*。在**最简单的情况下，当*A*只有一行，*B*只有一列时，矩阵乘法的结果就是这两个矩阵的点积。**

![单行矩阵与单列矩阵的乘法](image/transformer/matrix_mult_one_row_one_col.png)

注意，*A*中的列数和*B*中的行数必须相同，两个数组才能匹配，点积才能成立。

当矩阵*A*和*B* 的规模增大时，矩阵乘法就会变得复杂。为了处理矩阵*A*中的多行，需要分别计算矩阵*B*与每一行的点积。结果矩阵的行数将与矩阵*A* 的行数相同。

![两行矩阵与单列矩阵的乘法](image/transformer/matrix_mult_two_row_one_col.png)

当*B*包含更多列时，将每一列与*A*进行点积运算，并将结果堆叠在连续的列中。

![单行矩阵与两列矩阵的乘法](image/transformer/matrix_mult_one_row_two_col.png)

现在我们可以将此推广到任意两个矩阵的乘法，只要矩阵*A*的列数与矩阵*B*的行数相同。结果将与矩阵*A*的行数相同，与矩阵*B*的列数相同。

![一个一列三列矩阵与一个两列矩阵相乘](image/transformer/matrix_mult_three_row_two_col.png)

如果你是第一次看到这个，可能会觉得它过于复杂，但我保证以后会有回报的。

### 矩阵乘法作为查表

注意这里矩阵乘法是如何充当查找表的。我们的*A*矩阵由一叠独热向量组成，它们分别位于第一列、第四列和第三列，且元素均为 1。当我们进行矩阵乘法运算时，这相当于按顺序提取*B*矩阵的第一行、第四行和第三行。这种利用独热向量提取矩阵特定行的技巧正是 Transformer 函数的核心工作原理。

### 一阶序列模型

我们可以暂时抛开矩阵，回到我们真正关心的问题——词序列。想象一下，当我们开始开发自然语言计算机界面时，我们只想处理三种可能的命令：

- *Show me my directories please.*
- *Show me my files please*.
- *Show me my photos please*

我们的词汇量现在有七个：
{*directories, files, me, my, photos, please, show*}.



表示序列的一种有效方法是使用转换模型。对于词汇表中的每个词，它都能显示下一个可能的词是什么。如果用户一半时间询问照片，30% 时间询问文件，其余时间询问目录，那么转换模型将如下所示。从任何一个词出发的所有转换之和始终为 1。

![马尔可夫链转移模型](image/transformer/markov_chain.png)

这种特定的转移模型被称为**马尔可夫链**，因为它满足[马尔可夫性质](https://en.wikipedia.org/wiki/Markov_property)，即下一个词的概率仅取决于最近的词。更具体地说，它是一个一阶马尔可夫模型，因为它只考虑最近的单个词。如果它考虑最近的两个词，则它将是二阶马尔可夫模型。

我们已经告别了矩阵。事实证明，马尔可夫链可以方便地用矩阵形式表示。使用与创建独热向量时相同的索引方案，每一行代表词汇表中的一个词，每一列也是如此。矩阵转移模型将矩阵视为一个查找表。找到与你感兴趣的词对应的行。每一列的值表示该词接下来出现的概率。由于矩阵中每个元素的值都代表一个概率，因此它们的值都将介于 0 和 1 之间。由于概率之和始终为 1，因此每一行的值之和始终为 1。

![转移矩阵](image/transformer/transition_matrix.png)

从这个转移矩阵中，我们可以清晰地看到这三个句子的结构。几乎所有的转移概率都是0或1。马尔可夫链中只有一个分支点。在*“my”*之后，可能会出现*“directories”*、*“files”*或*“photos”*这几个词，每个词出现的概率都不同。除此之外，下一个词是什么是确定的。这种确定性体现在转移矩阵中大部分都是0和1。

我们可以再次利用矩阵乘法和独热向量来提取与给定单词相关的转移概率。例如，如果我们只想提取*“my”*之后出现哪个单词的概率，我们可以创建一个表示单词*“my”的*独热向量，并将其与转移矩阵相乘。这样就能提取出相关的行，并显示下一个单词的概率分布。

![转移概率查找](image/transformer/transition_lookups.png)

### 二阶序列模型

仅凭当前单词预测下一个单词很难。这就像只知道第一个音符就预测整首曲子的其余部分一样。如果我们至少能得到两个音符作为线索，成功的几率就会大大提高。

我们可以通过另一个用于计算机命令的玩具语言模型来了解它的工作原理。我们预计这个模型只会遇到两种句子，比例为 40/60。

- *Check whether the battery ran down please.*
- *Check whether the program ran please.*

马尔可夫链可以很好地说明这种一阶模型。



![另一个一阶马尔可夫链转移模型](image/transformer/markov_chain_2.png)

我们可以看到，如果我们的模型关注的是最近的两个词，而不是仅仅一个词，它的表现会更好。当遇到*“battery ran”*时，它就知道下一个词是*“down”*；当遇到*“program run”*时，它就知道下一个词是*“please”*。这消除了模型中的一个分支，降低了不确定性，提高了置信度。回溯两个词就将其转化为二阶马尔可夫模型。它提供了更多上下文信息来预测下一个词。二阶马尔可夫链的构建更具挑战性，但以下关系图展示了它们的价值。

![二阶马尔可夫链](image/transformer/markov_chain_second_order.png)

为了突出两者之间的区别，这里给出一阶转移矩阵：

![另一个一阶转移矩阵](image/transformer/transition_matrix_first_order_2.png)

这是二阶转移矩阵。

![二阶转移矩阵](image/transformer/transition_matrix_second_order.png)

注意，二阶矩阵的每一行都对应着一个词组（其中大部分词组未在此显示）。这意味着，如果初始词汇量为*N*，则转移矩阵将有*N²*行。

这让我们更有信心。在二阶模型中，1 的数量更多，分数的数量更少。模型中只有一行包含分数，只有一个分支。直观地说，观察两个词而不是一个词可以提供更多上下文信息，从而为猜测下一个词提供更多信息。

### 带跳跃的二阶序列模型

当只需要回溯两个词就能确定下一个词时，二阶模型效果很好。但如果需要回溯更远的词呢？假设我们正在构建另一个语言模型。这个模型只需要表示两个句子，每个句子出现的概率相等。

- *Check the program log and find out whether it ran please.*
- *Check the battery log and find out whether it ran down please.*



在这个例子中，为了确定*“ran”*后面应该跟哪个词，我们需要回溯到前8个词。如果我们想改进二阶语言模型，当然可以考虑三阶及更高阶的模型。然而，当词汇量较大时，这需要创造力和蛮力相结合才能实现。一个简单的八阶模型实现将有*N* ^8行，对于任何合理的词汇量来说，这都是一个荒谬的数字。

相反，我们可以巧妙地构建一个二阶模型，但只考虑最近出现的词与其之前每个词的组合。它仍然是二阶模型，因为我们每次只考虑两个词，但它允许我们追溯更远的词序，捕捉**长程依赖关系**。这种带跳跃的二阶模型与完整的n阶模型的区别在于，我们舍弃了大部分词序信息和先前词的组合。但剩下的信息仍然非常强大。

马尔可夫链现在完全失效了，但我们仍然可以表示每对前后词之间的联系。这里我们省略了数值权重，而只显示与非零权重相关的箭头。权重越大，用更粗的线条表示。

![带有跳跃特征的二阶投票](image/transformer/feature_voting.png)

在转移矩阵中，它可能看起来像这样。

![二阶跳跃转移矩阵](image/transformer/transition_matrix_second_order_skips.png)

*此视图仅显示与预测“ran”*之后出现的单词相关的行。它显示了词汇表中每个其他单词都位于最后一个单词“ *ran* ”之前的情况。仅显示相关值。所有空白单元格均为零。

首先显而易见的是，在预测*“ran”*之后的单词时，我们不再只关注单行，而是关注整组行。我们已经脱离了马尔可夫模型的范畴。每一行不再代表序列在特定点的状态，而是代表描述序列在特定点状态的众多**特征**之一。将最近出现的单词与其之前的每个单词组合起来，就构成了一组适用的行，可能数量庞大。由于这种含义的改变，矩阵中的每个值不再代表概率，而是代表投票。投票结果将被汇总并进行比较，以确定下一个单词的预测。

接下来显而易见的是，大多数特征都无关紧要。大多数单词在两个句子中都出现，因此它们出现过的事实对预测接下来要说什么没有任何帮助。它们的权重值均为 0.5。只有两个例外：*battery*和*program*。它们分别对应着我们试图区分的两种情况，权重分别为 1 和 0。特征*battery, ran*表示*ran*是最近出现的单词，而*battery出现在句子前面。该特征与**down*对应的权重为 1 ，与*please*对应的权重为 0。类似地，特征*program, ran 的*权重则相反。这种结构表明，决定下一个单词出现的是这两个单词在句子前面是否出现。

为了将这组词对特征转换为下一个词的预测，需要将所有相关行的值相加。向下累加列式时，“*检查程序日志并确定其是否运行”这一序列，除“* *down* ”为 4和*“please”为 5 外，其余所有词的总和均为 0。* *“检查电池日志并确定其是否运行”*这一序列也存在同样的问题，只是“ *down* ”为 5 ， *“please”*为 4。通过选择投票总数最高的词作为下一个词的预测，该模型即使存在八层词的依赖关系，也能给出正确的结果。

### 掩码

仔细考虑后，这令人难以满意。4票和5票之间的差距相对较小，这表明模型的置信度可能不够高。而且，在一个规模更大、更复杂的语言模型中，这种微小的差异很容易被统计噪声所掩盖。

我们可以通过剔除所有无信息特征投票来提高预测精度。除了*battery, ran* and *program, ran*这两个特征之外，其他特征都将被剔除。此时需要记住的是，我们通过将转移矩阵乘以一个表示当前激活特征的[向量，来提取相关行](#table_lookup)。在本例中，到目前为止，我们一直使用此处所示的隐含特征向量。

![特征选择向量](image/transformer/feature_selection.png)

它包含一个特征集，每个特征都是由单词*“ran”*与其前面的每个单词组合而成。任何在其后的单词都不会被包含在特征集中。（在接下来的单词预测问题中，这些单词尚未出现，因此用它们来预测接下来的单词是不公平的。）此外，这还不包括所有其他可能的单词组合。在本例中，我们可以忽略这些组合，因为它们的值都将为零。

**为了进一步提升结果，我们可以通过创建掩码**将无用特征强制置零。掩码是一个向量，除了需要隐藏或掩码的位置外，其余位置的值均为 1，而需要隐藏或掩码的位置的值则被置零。在本例中，我们希望掩蔽除*battery, ran*和*program, ran*之外的所有特征，因为只有这两个特征对我们有所帮助。

![掩饰特征活动](image/transformer/masked_feature_activities.png)

为了应用掩码，我们将两个向量逐元素相乘。未被掩码位置的任何特征活动值都将乘以 1 并保持不变。被掩码位置的任何特征活动值都将乘以 0，从而被强制设为零。

该掩码的作用是隐藏大部分转换矩阵。它隐藏了除*电池*和*程序*之外的所有*“运行”*组合，只保留了重要的特征。

![掩蔽转移矩阵](image/transformer/masked_transition_matrix.png)

在屏蔽掉无用特征后，下一个词的预测变得更加准确。当单词*battery*出现在句子前面时，*ran*之后的单词会被预测为*down*（权重为 1）和*please（*权重为 0）。原本 25% 的权重差异变成了无穷大百分比的差异。下一个词是什么毫无悬念。当*program*出现在句子前面时， *please 的*预测也同样准确。

这种选择性掩码过程正是Transformer原始[论文](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)标题中所提到的**注意力**机制。目前为止，我们所描述的只是对论文中注意力机制实现方式的一种近似描述。它抓住了重要的概念，但细节有所不同。我们稍后会弥补这些差异。

### 休息站和出口匝道

恭喜你读到这里。如果你想停下来，随时可以。选择性二阶带跳跃模型是一种思考Transformer模型工作原理的有效方法，至少在解码器方面是如此。它在一定程度上捕捉到了OpenAI的[GPT-3](https://en.wikipedia.org/wiki/GPT-3)等生成式语言模型的工作原理。虽然它并不能涵盖全部内容，但它代表了其核心要义。

接下来的章节将进一步探讨这种直观解释与Transformer实际实现方式之间的差距。这些内容主要受三个实际因素的驱动。

1. **计算机尤其擅长矩阵乘法。**围绕着专门用于快速矩阵乘法的计算机硬件，已经形成了一个完整的产业。任何可以用矩阵乘法表达的计算都可以变得异常高效。它就像一列高速列车。如果你能把行李装上去，它就能以最快的速度把你送到目的地。
2. **每一步都必须是可微的。**目前为止，我们只是使用了简单的示例，并且可以随意选择所有的转移概率和掩码值——也就是模型**参数**。但在实际应用中，这些参数必须通过**反向传播**来学习，而反向传播依赖于每一步计算的可微性。这意味着，对于参数的任何微小变化，我们都可以计算出模型误差或**损失的**相应变化。
3. **梯度需要平滑且条件良好。**所有参数导数的组合就是损失**梯度**。在实践中，反向传播算法要想表现良好，就需要平滑的梯度，也就是说，无论你朝哪个方向迈出小步，斜率都不会变化太快。当梯度条件良好时，算法的表现也会更好，也就是说，梯度在一个方向上的幅度不会远大于另一个方向。如果把损失函数想象成一幅风景，那么大峡谷就是一个条件不良的风景。根据你是沿着谷底行进还是沿着峡谷壁向上攀登，你将遇到截然不同的坡度。相比之下，经典Windows屏幕保护程序的起伏山丘则具有条件良好的梯度。
   如果说神经网络架构的科学在于创建可微分的构建模块，那么它的艺术就在于以某种方式堆叠这些模块，使得梯度变化不快，并且在各个方向上的大小大致相同。



### 注意力如同矩阵乘法

特征权重可以通过统计训练过程中每个词对/下一个词转换出现的频率来轻松构建，但注意力掩码却并非如此。到目前为止，我们都是凭空捏造掩码向量的。Transformer 如何找到相关的掩码至关重要。使用某种查找表是很自然的做法，但现在我们专注于将所有运算都表示为矩阵乘法。我们可以采用之前介绍的[查找](https://brandonrohrer.com/table_lookup)方法，将每个词的掩码向量堆叠成一个矩阵，然后使用最近一个词的独热编码来提取相关的掩码。

![通过矩阵乘法进行掩码查找](image/transformer/mask_matrix_lookup.png)

为了清晰起见，在显示掩码向量集合的矩阵中，我们只显示了我们正在尝试提取的那个掩码向量。

我们终于可以开始把内容与论文联系起来了。这种掩码查找由注意力方程中的*QK^T项表示。*

![注意力方程突出了QKT](image/transformer/attention_equation_QKT.png)

查询*Q*代表感兴趣的特征，矩阵*K*代表掩码集合。由于掩码是按列而不是行存储的，因此在进行乘法运算之前需要进行转置（使用*T*运算符）。最终我们会对此进行一些重要的修改，但目前它已经体现了 Transformer 所使用的可微查找表的概念。

### 二阶序列模型作为矩阵乘法

另一个我们之前一直含糊其辞的步骤是转移矩阵的构建。我们已经明确了其逻辑，但没有明确如何用矩阵乘法来实现。

一旦我们得到了注意力步骤的结果——一个包含最新出现的词以及其前面一小部分词的向量——我们就需要将其转换为特征，每个特征都是一个词对。注意力掩蔽可以提供我们需要的原始数据，但它并不能构建这些词对特征。为此，我们可以使用单层全连接神经网络。

为了了解神经网络层如何创建这些词对，我们将手动构建一个示例。该示例将经过精心设计，使其更加简洁美观，其权重与实际应用中的权重截然不同，但它将展示神经网络如何具备构建这两个词对特征所需的表达能力。为了保持示例简洁明了，我们将仅关注此示例中的三个重点词：*battery*、*program*和*ran*。

![用于创建多词特征的神经网络层](image/transformer/feature_creation_layer.png)

在上图中，我们可以看到权重是如何将每个词的存在与否组合成一组特征的。这也可以表示为矩阵形式。

![用于创建多词特征的权重矩阵](image/transformer/feature_creation_matrix.png)

可以通过矩阵乘法与表示目前为止所看到的单词集合的向量来计算。

![“电池运行时间”功能的计算](image/transformer/second_order_feature_battery.png)

电池和*运行*元素均为 1，*程序*元素为 0。*偏置元素始终为 1，这**是神经网络的一个特性。通过矩阵乘法运算，得到代表**电池和运行的*元素为 1，代表*程序和运行的*元素为 -1 。其他情况的结果类似。

![“程序运行”功能的计算](image/transformer/second_order_feature_program.png)

计算这些词组合特征的最后一步是应用修正线性单元（ReLU）非线性激活函数。其作用是将所有负值替换为零。这使得结果更加清晰，能够分别表示每个词组合特征的存在（1）或缺失（0）。

经过一番摸索，我们终于得到了一种基于矩阵乘法的多词特征创建方法。虽然我最初声称这些特征由最近出现的词和之前出现的一个词组成，但仔细研究这种方法后发现，它还可以构建其他类型的特征。当特征创建矩阵是学习得到的，而不是硬编码的，那么就可以学习到其他结构。即使在这个简单的例子中，也完全可以创建像“ *battery”、“program”、“ran”这样的三词组合。如果这种组合出现的频率足够高，它很可能最终会被表示出来。虽然*[目前](#positional_encoding)还无法确定这些词的出现顺序，但我们完全可以利用它们的共现来进行预测。甚至可以利用忽略最近出现的词的组合，例如*“battery”、“program”*。这些以及其他类型的特征很可能在实践中被创建出来，这也暴露了我之前关于Transformer模型是选择性二阶带跳跃序列模型的说法过于简单化。事情远不止如此，现在你应该能明白其中的细微差别了。这不会是我们最后一次修改故事，使其更具深度。

在这种情况下，多词特征矩阵可以进行另一次矩阵乘法运算，即我们[上面](https://brandonrohrer.com/second_order_skips)开发的带跳跃的二阶序列模型。总而言之，序列为

- 特征创建矩阵乘法，
- ReLU非线性，以及
- 转移矩阵乘法

这些是注意力机制应用之后执行的前馈处理步骤。论文中的公式 2 以简洁的数学公式展示了这些步骤。



![前馈模块背后的方程式](image/transformer/feedforward_equations.png)

本文的图 1 架构图显示，这些模块被组合在一起，构成前馈模块。

![Transformer架构图，展示了前馈模块](image/transformer/architecture_feedforward.png)

### 序列完成

到目前为止，我们只讨论了下一个词的预测。要让解码器生成长序列，还需要添加一些组件。首先是**提示**，也就是一些示例文本，它能为转换器提供启动和上下文，以便构建序列的其余部分。提示会被输入到解码器中，也就是上图中右侧的“输出（右移）”列。选择一个能生成有趣序列的提示本身就是一门艺术，称为提示工程。这也很好地体现了人类为了适应算法而改变自身行为，而不是反过来。

一旦解码器获得一个部分序列作为初始状态，它就会进行前向传递。最终结果是一组单词的预测概率分布，序列中的每个位置对应一个概率分布。在每个位置，该分布显示了词汇表中每个后续单词的预测概率。我们并不关心序列中每个已存在单词的预测概率，因为它们已经确定。我们真正关心的是提示结束后的下一个单词的预测概率。选择该单词的方法有很多种，但最直接的方法称为**贪婪算法**，即选择概率最高的单词。

然后，将下一个新词添加到序列中，替换解码器底部的“输出”，然后重复此过程，直到你厌倦为止。

我们目前还无法详细描述的一项技术是另一种掩码形式，它确保 Transformer 在进行预测时只关注过去，而不关注未来。这项技术应用于名为“掩码多头注意力”的模块中。稍后我们会更详细地介绍它的实现方式。

### 嵌入

正如我们目前所描述的，Transformer 矩阵过于庞大。假设词汇量*N*为 50,000，那么所有词对及其所有潜在下一个词之间的转换矩阵将有 50,000 列和 50,000 平方（25 亿）行，总计超过 100 万亿个元素。即使对于现代硬件而言，这仍然是一个巨大的挑战。

问题不仅仅在于矩阵的大小。为了构建一个稳定的转换语言模型，我们需要提供训练数据，至少要多次展示每一种可能的序列。这远远超出了即使是最雄心勃勃的训练数据集的容量。

幸运的是，这两个问题都有解决办法，那就是嵌入。

在语言的独热编码表示中，每个词都对应一个向量元素。对于大小为*N*的词汇表，该向量构成一个*N*维空间。每个词代表该空间中的一个点，该点沿多个轴之一距离原点一个单位。我还没找到绘制高维空间的最佳方法，但下面是一个粗略的表示。

![img](image/transformer/one_hot_vs_embedding.png)

在词嵌入中，所有词点都被提取出来，并重新排列（用线性代数术语来说是**投影**）到一个低维空间中。上图展示了它们在二维空间中的样子。现在，我们不再需要*N 个*数字来指定一个词，只需要 2 个。这些是每个点在新空间中的 ( *x* , *y* ) 坐标。下图展示了我们示例的二维词嵌入，以及一些词的坐标。

![img](image/transformer/embedded_words.png)

好的词嵌入会将含义相近的词分组在一起。使用词嵌入的模型会学习嵌入空间中的模式。这意味着，它对一个词的学习结果会自动应用到与其相邻的所有词上。这样做还有一个额外的好处，那就是减少了所需的训练数据量。每个示例都提供了一些学习信息，这些信息会被应用到整个词邻域中。

在这个例子中，我试图通过将重要组成部分放在一个区域（*battery*、*log*、*program*），介词放在另一个区域（*down*、*out*），动词放在中心附近（*check*、*find*、*ran*）来展示这一点。在实际的词嵌入中，分组可能不那么清晰或直观，但其基本概念是相同的：行为相似的词语之间的距离很小。

词嵌入可以大幅减少所需的参数数量。然而，嵌入空间的维度越低，丢失的原始词信息就越多。语言的丰富性仍然需要相当大的空间来布局所有重要的概念，以避免它们相互干扰。通过选择合适的嵌入空间大小，我们可以在计算量和模型精度之间进行权衡。

你可能不会感到惊讶，将单词从独热编码表示投影到嵌入空间的过程涉及矩阵乘法。投影正是矩阵最擅长的运算。从一个具有一行*N*列的独热编码矩阵开始，投影到二维嵌入空间时，投影矩阵将具有*N*行两列，如下图所示。

![描述嵌入的投影矩阵](image/transformer/embedding_projection.png)

这个例子展示了如何利用一个独热向量（例如表示*“battery”*）提取与其关联的行，该行包含单词在嵌入空间中的坐标。为了更清晰地展示这种关系，独热向量中的零值被隐藏，投影矩阵中所有未被提取的行也被隐藏。完整的投影矩阵是稠密的，每一行都包含与其关联的单词的坐标。

投影矩阵可以将原始的独热词汇向量集合转换成任意维度空间中的任意配置。关键在于找到一个有效的投影，它既能将相似的词分组在一起，又能提供足够的维度来分散这些词。对于英语等常用语言，已经有一些不错的预计算词嵌入。此外，就像Transformer模型中的其他所有组件一样，它也可以在训练过程中学习。

在原论文的图 1 架构图中，嵌入就发生在这里。

![Transformer架构图，展示了嵌入模块](image/transformer/architecture_embedding.png)

### 位置编码

到目前为止，我们一直假设词的位置信息被忽略，至少对于最后一个词之前的词来说是这样。现在，我们将使用位置嵌入来解决这个问题。

有几种方法可以将位置信息引入到我们对单词的嵌入式表示中，但原始 Transformer 中实现的方法是添加一个圆形摆动。

![位置编码引入了圆形摆动](image/transformer/positional_encoding.png)

词在嵌入空间中的位置相当于一个圆心。根据词在词序列中的位置，会对其施加一个扰动。对于每个位置，词都会移动相同的距离，但角度不同，从而在词序列中形成一个圆形图案。序列中彼此靠近的词具有相似的扰动，而相距较远的词则具有不同方向的扰动。

由于圆是二维图形，表示圆周摆动需要修改嵌入空间的两个维度。如果嵌入空间包含两个以上的维度（这种情况几乎总是如此），则圆周摆动会在所有其他维度对中重复出现，但角频率不同，也就是说，在每种情况下，摆动的旋转次数不同。在某些维度对中，摆动会旋转圆周很多圈；而在另一些维度对中，摆动只会旋转一小部分圈数。所有这些不同频率的圆周摆动组合起来，就能很好地表示单词在序列中的绝对位置。

我仍在努力理解这种方法的原理。它似乎以一种不会破坏词语与注意力之间已建立联系的方式，将位置信息融入其中。如果想更深入地了解其中的数学原理和应用，我推荐阅读 Amirhossein Kazemnejad 的位置编码[教程](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)。

在规范架构图中，这些模块显示了位置代码的生成及其添加到嵌入词的过程。

![Transformer架构展示了位置编码](image/transformer/architecture_positional.png)

### 反嵌入

词嵌入极大地提高了处理效率，但处理结束后，需要将词转换回原始词汇表中的单词。反嵌入与词嵌入的方法相同，都是通过从一个空间投影到另一个空间，也就是矩阵乘法来实现的。

反嵌入矩阵的形状与嵌入矩阵相同，只是行数和列数互换了。行数代表我们要转换的源空间的维度。在我们一直使用的例子中，它就是嵌入空间的大小，即 2。列数代表我们要转换的目标空间的维度——完整词汇表的独热编码表示的大小，在我们的例子中是 13。

![去嵌入变换](image/transformer/de_embedding.png)

一个好的反嵌入矩阵中的值不像嵌入矩阵中的值那样容易直观地表示，但效果类似。例如，当一个表示单词“ *program”的*嵌入向量乘以去嵌入矩阵时，对应位置的值会很高。然而，由于投影到更高维空间的原理，其他单词对应的值不会为零。在嵌入空间中与*“program”*最接近的单词也会具有中高值。其他单词的值接近于零。而且很可能有很多单词的值是负数。词汇空间中的输出向量不再是独热编码或稀疏的，而是稠密的，几乎所有值都非零。

![反嵌入的代表性稠密结果向量](image/transformer/de_embedded_results.png)

没关系。我们可以通过选择与最大值关联的单词来重新生成独热向量。这个操作也称为**argmax** ，即选择具有最大值的参数（元素）。这就是[上面](#sequence_completion)提到的贪婪序列补全方法。这是一个很好的初步尝试，但我们可以做得更好。

如果一个词嵌入能够很好地映射到多个词，我们未必每次都选择最佳词。它可能只是比其他词略好一些，而增加一些多样性可以让结果更有趣。此外，有时提前考虑几个词，思考句子可能的各种走向，然后再最终确定选择，也是很有用的。为了实现这些，我们首先需要将反嵌入结果转换为概率分布。

### Softmax

argmax 函数之所以被称为“硬”函数，是因为它会选择最大值，即使它只比其他值大无穷小。如果我们想同时考虑多种可能性，最好使用“软”最大值函数，即**softmax 函数**。要计算向量中值*x的 softmax 值，需要将**x*的指数*e^x*除以向量中所有值的指数之和。

softmax 在这里很有用，原因有三。首先，它将我们的去嵌入结果向量从任意值集合转换为概率分布。作为概率，我们可以更轻松地比较不同词被选中的可能性，甚至可以比较多词序列被选中的可能性，以便进行更深入的预测。

其次，它会缩小排名靠前的单词的范围。如果某个单词的得分明显高于其他单词，softmax 会放大这种差异，使其看起来几乎像 argmax 一样，获胜单词的值接近 1，而其他单词的值接近 0。但是，如果有多个单词的得分都非常接近，softmax 会将它们都保留为高概率单词，而不是人为地削弱排名接近的第二名。

第三，softmax 函数是可微的，这意味着我们可以计算出，当输入元素发生微小变化时，结果中每个元素会发生多大的变化。这使得我们可以将其与反向传播算法结合使用，来训练我们的 Transformer 模型。

如果你想深入了解 softmax 技术（或者如果你晚上难以入睡），这里有一篇更完整的[文章](https://brandonrohrer.com/softmax.html)。

去嵌入变换（如下图所示的线性块）和 softmax 函数共同完成了去嵌入过程。

![Transformer架构展示了去嵌入过程](image/transformer/architecture_de_embedding.png)

### 多头注意力

既然我们已经理解了投影（矩阵乘法）和空间（向量大小）的概念，就可以重新审视核心注意力机制了。如果我们能更具体地描述每个阶段矩阵的形状，将有助于我们更清晰地理解算法。这里有一个包含一些重要数值的列表。

- *N*：词汇量。在本例中为 13。通常情况下，词汇量在数万以内。
- *n*：最大序列长度。在我们的示例中为 12。论文中大约是几百。（他们没有具体说明。）GPT-3 中为 2048。
- *d_model*：模型中使用的嵌入空间的维度数。论文中为 512。



原始输入矩阵的构造方法是：将句子中的每个单词转换为独热编码，然后将它们堆叠起来，使得每个独热向量都成为其自身的一行。得到的输入矩阵有*n*行和*N*列，我们可以将其简写为 [ *n* x *N* ]。

![矩阵乘法会改变矩阵的形状](image/transformer/matrix_multiply_shape.png)

如前所述，嵌入矩阵有*N*行和*d_model*列，我们可以将其简写为 [ *N* x *d_model* ]。当两个矩阵相乘时，结果的行数取自第一个矩阵，列数取自第二个矩阵。这使得嵌入的词序列矩阵的形状为 [ *n* x *d_model* ]。

我们可以通过追踪Transformer模型中矩阵形状的变化来了解发生了什么。初始嵌入之后，位置编码是加法运算而非乘法运算，因此不会改变矩阵的形状。然后，嵌入后的词序列进入注意力层，并以相同的形状从另一端输出。（我们稍后会详细介绍这些内部机制。）最后，反嵌入将矩阵恢复到其原始形状，为词汇表中的每个词在序列的每个位置提供一个概率值。

![变压器模型中的矩阵形状](image/transformer/matrix_shapes.png)

### 为什么我们需要不止一个注意力头

现在终于到了正视我在第一次解释注意力机制时所做的一些过于简单化的假设的时候了。词语是用稠密嵌入向量表示的，而不是独热向量。注意力并非只有 1 或 0，开或关两种状态，而是介于两者之间的任何状态。为了使结果落在 0 到 1 之间，我们再次使用了 softmax 技巧。它有两个好处：一是强制所有值都落在 [0, 1] 的注意力范围内；二是能够突出最高值，同时大幅降低最低值。这正是我们在之前解释模型最终输出时所利用的近似 argmax 的差异化行为。

将softmax函数放入注意力机制的一个复杂后果是，它往往会聚焦于单个元素。这是我们之前没有遇到的限制。有时，在预测下一个词时，记住前面的几个词会很有用，而softmax函数恰恰剥夺了我们这种能力。这对模型来说是个问题。

解决方案是同时运行多个不同的注意力机制，或者说多个“**注意力头”**。这样，Transformer 在预测下一个词时就可以同时考虑之前的几个词。它恢复了我们在引入 softmax 之前所拥有的强大功能。

不幸的是，这样做会显著增加计算量。注意力机制的计算本身就已经是工作量最大的部分了，而我们只是将其乘以想要使用的“脑”的数量。为了解决这个问题，我们可以再次利用将所有数据投影到低维嵌入空间的技巧。这可以缩小涉及的矩阵规模，从而显著减少计算时间。问题解决了。

为了了解其运作方式，我们可以继续观察矩阵形状。追踪多头注意力模块的分支和交织结构中的矩阵形状，还需要三个数字。

- *d_k*：用于键和查询的嵌入空间中的维度。论文中为 64。
- *d_v*：用于值的嵌入空间中的维度。论文中为 64。
- *h*：正面朝上的次数。论文中为 8。



![变形金刚架构展现多头注意力](image/transformer/architecture_multihead.png)

嵌入词的[ *n* x *d_model* ] 序列是后续所有操作的基础。在每种情况下，都存在一个矩阵*Wv*、*Wq*和*Wk*（在架构图中均以“线性”块的形式显示，这不太便于理解），它们将原始嵌入词序列转换为值矩阵*V*、查询矩阵*Q*和键矩阵K。K*和*Q*的**形状*相同，均为 [ *n* x *d_k* ]，但*V 的形状*可以不同，例如 [ *n* x *d_v* ]。论文中*d_k*和*d_v*相同，这可能会造成一些混淆，但它们并非必须相同。此设置的一个重要方面是，每个注意力头都有其自身的*Wv*、*Wq*和*Wk*变换。这意味着每个注意力头都可以放大和扩展其想要关注的嵌入空间部分，并且其关注的内容可以与其他注意力头不同。

*每个注意力头的结果都与V 的*形状相同。现在我们面临*h 个*不同的结果向量的问题，每个向量关注序列中的不同元素。为了将它们合并成一个向量，我们利用线性代数的强大功能，将所有这些结果连接成一个巨大的 [ *n* x *h \* d_v* ] 矩阵。然后，为了确保它最终的形状与初始形状相同，我们使用形状为 [ *h \* d_v* x *d_model* ] 的变换。

以上就是全部内容的简要概括。

![论文中的多头注意力方程](image/transformer/multihead_attention_equation.png)

### 单头注意力再探

我们之前已经通过概念图展示了注意力机制。实际实现起来稍微复杂一些，但我们之前的直觉仍然很有帮助。查询和键不再容易检查和解释，因为它们都被投影到各自独特的子空间中。[在](#attention)我们的概念图中，查询矩阵中的每一行代表词汇空间中的一个点，由于采用了独热编码，每个点代表且仅代表一个词。在嵌入空间中，查询矩阵中的每一行代表嵌入空间中的一个点，该点靠近一组含义和用法相似的词。概念图将一个查询词映射到一组键，这些键过滤掉了所有未被关注的值。在实际实现中，每个注意力头将一个查询词映射到另一个低维嵌入空间中的一个点。这样一来，注意力就变成了词组之间的关系，而不是单个词之间的关系。它利用语义相似性（嵌入空间中的接近程度）来概括它所学习到的关于相似词的信息。

通过观察注意力计算过程中矩阵的形状，有助于跟踪其运行情况。

![变形金刚架构展现单头注意力](image/transformer/architecture_single_head.png)

查询矩阵*Q*和键矩阵*K*的形状均为 [ *n* x *d_k* ]。由于*K*在乘法运算前进行了转置，*QK^T*的结果是一个 [ *n* x *d_k* ] * [ *d_k* x *n* ] = [ *n* x *n ] 的矩阵。将该矩阵的每个元素除以**d_k*的平方根已被证明可以防止数值的幅度急剧增长，并有助于反向传播算法的良好性能。如前所述，softmax 函数将结果近似为 argmax 函数，倾向于将注意力集中在序列中的某个元素上。在这种形式下，[ *n* x *n* ] 的注意力矩阵大致将序列中的每个元素映射到序列中的另一个元素，指示它应该关注什么才能获得预测下一个元素的最相关上下文。它最终会应用于值矩阵*V*，只留下被关注的值的集合。这样就忽略了序列中前面的大部分内容，并将焦点集中在最需要注意的前面元素上。

![注意力方程](image/transformer/attention_equation.png)

理解这组计算的关键在于要记住，它计算的是输入序列中每个元素（句子中的每个词）的注意力，而不仅仅是最近出现的词。它还会计算前面词的注意力。我们其实并不关心这些词，因为它们的下一个词已经被预测并确定了。它还会计算后面词的注意力。这些词目前用处不大，因为它们出现的时间太远，而且它们的直接前导词尚未被选中。但是，这些计算结果可以通过一些间接路径影响最近出现的词的注意力，所以我们把它们都考虑在内。只是在计算序列中每个位置的词概率时，我们会舍弃大部分概率，只关注下一个词。

Mask 模块强制执行一个约束：至少对于这个序列补全任务而言，我们不能窥探未来。它避免引入任何由虚构的未来词语引起的奇怪现象。这种方法虽然粗糙但有效——手动将当前位置之后所有词语的注意力值设置为负无穷大。在[《带注释的 Transformer》](https://nlp.seas.harvard.edu/2018/04/03/attention.html)中，作者以极其有用的方式逐行展示了 Python 实现，并可视化了 Mask 矩阵。紫色单元格表示禁止关注的区域。每一行对应序列中的一个元素。第一行只允许关注自身（第一个元素），不允许关注其后的任何内容。最后一行允许关注自身（最后一个元素）以及其之前的所有元素。Mask 是一个 [ *n* x *n* ] 矩阵。它并非通过矩阵乘法应用，而是通过更直接的逐元素乘法来实现。这相当于手动进入注意力矩阵，并将 Mask 中所有紫色的元素值设置为负无穷大。

![用于序列补全的注意力掩码](image/transformer/mask.png)

注意力机制实现方式的另一个重要区别在于，它利用了单词在序列中的呈现顺序，并将注意力表示为位置关系而非词与词之间的关系。这一点在其 [ *n* x *n* ] 矩阵的形状中显而易见。它将序列中的每个元素（由行索引指示）映射到序列中的其他元素（由列索引指示）。由于它是在嵌入空间中运行的，因此有助于我们更轻松地可视化和解释其工作原理。我们无需额外步骤在嵌入空间中查找邻近词来表示查询和键之间的关系。

### 跳过连接

注意力是变压器运作的最根本组成部分。它是核心机制，我们现在已经相当深入地了解了它。接下来的一切都是为了使其有效运行而必要的配套设施。正是这些其他环节，才使得注意力能够帮助我们应对繁重的工作。

我们尚未解释的一个部分是跳跃连接。这些连接出现在多头注意力模块周围，以及标记为“加法”和“归一化”的模块中的逐元素前馈模块周围。在跳跃连接中，输入的副本会添加到一组计算的输出中。注意力模块的输入会被加回其输出。逐元素前馈模块的输入会被加到其输出中。

![Transformer架构图，展示了加法和归一化模块](image/transformer/architecture_add_norm.png)

跳过连接有两个用途。

首先，它们有助于保持梯度平滑，这对反向传播至关重要。注意力机制本质上是一个过滤器，这意味着当它正常工作时，它会阻挡大部分试图通过它的输入。其结果是，如果输入恰好落入被阻塞的通道，那么许多输入的微小变化可能不会对输出产生显著影响。这会在梯度中产生“死点”，这些死点虽然平坦，但距离谷底还很远。这些鞍点和脊点是反向传播的一大障碍。跳跃连接有助于平滑这些死点。对于注意力机制而言，即使所有权重都为零且所有输入都被阻塞，跳跃连接也会将输入的副本添加到结果中，从而确保任何输入的微小变化仍然会对结果产生显著影响。这可以防止梯度下降算法在远离理想解的地方停滞不前。

自 ResNet 图像分类器问世以来，跳跃连接因其显著的性能提升而广受欢迎。如今，它已成为神经网络架构中的一项标准特性。我们可以通过比较带有和不带有跳跃连接的网络来直观地了解其效果。下图（来自本文）展示了带有和不带有跳跃连接的 ResNet 网络。使用跳跃连接后，损失函数曲线的斜率更加平缓均匀。如果您想深入了解其工作原理和原因，可以阅读[这篇](https://theaisummer.com/skip-connections/)，其中有更详细的阐述。

![比较有跳跃连接和无跳跃连接的损耗面](image/transformer/skip_connection_gradients.png)

跳跃连接的第二个用途是Transformer特有的——保留原始输入序列。即使使用大量的注意力头，也无法保证每个词都会关注到自身的位置。注意力过滤器有可能完全忽略最近的词，转而关注所有可能相关的先前词。跳跃连接会将原始词手动添加回信号中，从而避免它被丢弃或遗忘。这种鲁棒性或许是Transformer在众多不同的序列补全任务中表现良好的原因之一。

### 层归一化

归一化是与跳跃连接配合使用效果很好的步骤。虽然它们并非必须同时使用，但当它们置于一组计算之后（例如注意力机制或前馈神经网络）时，都能发挥最佳效果。

层归一化的简短版本是将矩阵的值进行平移，使其均值为零，并进行缩放，使其标准差为一。

![多个分布被归一化](image/transformer/normalization.png)

更详细地说，在像Transformer这样的系统中，由于存在许多动态组件，其中一些并非简单的矩阵乘法（例如softmax算子或修正线性单元），因此数值的大小以及正负值的平衡至关重要。如果一切都是线性的，你可以将所有输入值翻倍，输出值也会翻倍，一切都能正常运行。但神经网络并非如此。它们本质上是非线性的，这使得它们具有很强的表达能力，但也对信号的幅值和分布非常敏感。归一化是一种已被证明有效的技术，它能够确保多层神经网络中每一步的信号值分布保持一致。它有助于参数值的收敛，通常能显著提升网络性能。

我最喜欢归一化的一点是，除了像我刚才那样给出的概括性解释之外，没有人能完全确定它为什么如此有效。如果你想更深入地了解它，我写了一篇更详细的关于批处理归一化的[文章](https://brandonrohrer.com/batch_normalization.html)，它是Transformer模型中使用的层归一化的近亲。

### 多层

在我们构建上述基础模型时，我们已经证明，一个注意力模块和一个权重经过精心选择的前馈模块足以构建一个不错的语言模型。在我们的示例中，大多数权重为零，少数为一，而且所有权重都是我们精心挑选的。但当我们使用原始数据进行训练时，就没有这种便利了。初始阶段，所有权重都是随机选择的，大多数接近于零，而少数非零的权重可能也不是我们需要的。这与模型达到良好性能所需的状态相去甚远。

通过反向传播实现的随机梯度下降法可以做出一些非常惊人的结果，但它很大程度上依赖于运气。如果只有一条通往正确答案的路径，或者只有一套权重组合才能使网络有效工作，那么它找到正确答案的可能性就很小。但如果有很多条路径可以到达一个好的解决方案，那么模型找到正确答案的概率就会大大提高。

仅使用一个注意力层（一个多头注意力模块和一个前馈模块）意味着只有一条路径可以找到一组合适的Transformer参数。每个矩阵的每个元素都需要找到正确的值才能保证模型正常工作。这种模型非常脆弱，除非初始参数猜测非常幸运，否则很容易陷入远非理想的解中。

Transformer 规避这个问题的方法是使用多个注意力层，每个注意力层都使用前一个注意力层的输出作为其输入。跳跃连接的使用使得整个流程能够很好地应对单个注意力模块的故障或异常结果。多个注意力层意味着有其他注意力层可以随时弥补不足。如果某个注意力层出现故障或未能发挥其应有的性能，下游的注意力层将有机会弥补差距或纠正错误。论文表明，更多的注意力层可以带来更好的性能，但超过 6 层后，性能提升变得微乎其微。

另一种理解多层模型的方式是将其想象成一条传送带装配线。每个注意力模块和前馈模块都有机会从传送带上取下输入，计算有用的注意力矩阵并进行下一个词的预测。无论结果是否有效，都会被重新添加到传送带上，并传递到下一层。

![变压器重新绘制成传送带](image/transformer/layer_conveyer.png)

这与传统上将多层神经网络描述为“深度”神经网络的做法截然不同。由于跳跃连接的存在，后续层提供的并非越来越复杂的抽象，而是冗余。某一层错失的集中注意力、创建有用特征和进行准确预测的机会，总能被下一层弥补。各层就像装配线上的工人，各自尽职尽责，无需担心是否能处理完所有环节，因为下一层会弥补它们的不足。

### 解码器栈

到目前为止，我们一直刻意忽略编码器栈（Transformer架构的左侧部分），而只关注解码器栈（右侧部分）。稍后我们会对此进行修正。但值得注意的是，解码器本身也相当有用。

正如我们在[序列补全](#sequence_completion)任务描述中所述，解码器可以补全部分序列，并根据需要将其扩展。OpenAI 创建了生成式预训练 (GPT) 模型系列来实现这一目标。他们在本[报告](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)中描述的架构应该看起来很熟悉。它是一个 Transformer 模型，但编码器堆栈及其所有连接都被移除。剩下的是一个 12 层的解码器堆栈。

![GPT系列模型的架构](image/transformer/gpt_architecture.png)

每当你遇到像[BERT](https://arxiv.org/pdf/1810.04805v2.pdf)、[ELMo](https://arxiv.org/abs/1802.05365)或[Copilot](https://copilot.github.com/)这样的生成模型时，你看到的很可能就是 Transformer 模型的解码器部分在运行。

### 编码器堆栈

我们对解码器所了解的几乎所有内容也适用于编码器。最大的区别在于，编码器最终不会做出任何明确的预测，因此我们无法用这些预测来判断其性能的正确性。相反，编码器栈的最终产物令人失望地抽象——嵌入空间中的一系列向量。有人将其描述为序列的纯粹语义表示，脱离了任何特定的语言或词汇，但我感觉这种说法过于理想化。我们确切知道的是，它是一个有用的信号，可以向解码器栈传达意图和含义。

编码器栈的加入，使得Transformer模型能够充分发挥其潜力，不再仅仅生成序列，而是可以将序列从一种语言翻译（或转换）成另一种语言。翻译任务的训练与序列补全任务的训练有所不同。训练数据需要源语言的序列和目标语言的匹配序列。源语言的完整序列会经过编码器处理（这次没有掩码，因为我们假设在生成翻译之前可以看到整个句子），编码器的最终输出会作为每个解码器层的输入。然后，解码器中的序列生成过程与之前相同，但这次不需要任何提示来启动它。

### 交叉注意力

让整个Transformer系统正常运行的最后一步是连接编码器和解码器堆栈，也就是交叉注意力模块。我们把它留到了最后，而且由于之前已经做了充分的准备工作，所以剩下的部分也无需过多解释。

交叉注意力机制的工作原理与自注意力机制类似，区别在于其键矩阵*K*和值矩阵*V*基于最终编码器层的输出，而非前一个解码器层的输出。查询矩阵*Q*仍然基于前一个解码器层的输出计算得出。正是通过这种机制，源序列的信息得以传递到目标序列中，并引导其朝着正确的方向发展。值得注意的是，解码器的每一层都使用相同的嵌入源序列，这体现了连续各层之间的冗余性，以及它们协同完成同一任务的理念。

![Transformer架构展示了交叉注意力模块](image/transformer/architecture_cross_attention.png)

### 分词

我们已经完整地讲解了Transformer模型！讲解得非常详细，应该不会再有任何神秘的黑盒子了。不过，还有一些实现细节我们没有深入探讨。你需要了解这些细节才能自己构建一个可运行的版本。最后这些细节与其说是关于Transformer的工作原理，不如说是关于如何让神经网络表现良好。[带注释的Transformer模型](https://nlp.seas.harvard.edu/2018/04/03/attention.html)将帮助你填补这些空白。

不过，我们还没完全完成。关于如何呈现数据，还有一些重要的事情需要说明。这个话题对我来说意义重大，但也很容易被忽视。它与其说是算法的强大之处，不如说是思考如何解读数据并理解其含义。

我们之前提到过，词汇表可以用高维独热向量表示，每个元素对应一个词。为了做到这一点，我们需要确切地知道要表示多少个词以及它们分别是什么。

一种简单粗暴的方法是列出所有可能的单词，就像韦氏词典那样。对于英语来说，这会列出数万个单词，具体数量取决于我们选择包含或排除哪些单词。但这是一种过于简化的做法。大多数单词都有多种形式，包括复数、所有格和词形变化。单词也可能有不同的拼写方式。而且，除非你的数据经过非常仔细的清理，否则其中必然包含各种各样的拼写错误。这甚至还没有考虑到自由文本、新词、俚语、行话以及庞大的Unicode编码体系所带来的可能性。列出所有可能的单词将极其庞大，几乎不可能实现。

一个合理的备选方案是使用单个字符而非单词作为构建单元。我们完全有能力计算出所有字符的完整列表。然而，这种方案存在一些问题。在将数据转换到嵌入空间后，我们假设该空间中的距离具有语义解释，也就是说，我们假设距离较近的点具有相似的含义，而距离较远的点则具有截然不同的含义。这使得我们可以隐式地将对一个词的理解推广到其相邻词，而我们正是依赖这一假设来提高计算效率，Transformer 也正是基于此获得了一定的泛化能力。

在单个字符层面，语义内容非常少。例如，英语中有一些单字符词，但数量不多。表情符号是个例外，但它们并非我们所研究的大多数数据集的主要内容。这就导致我们陷入了一个令人遗憾的境地：词嵌入空间并不实用。

理论上，如果我们能够找到足够丰富的字符组合来构建语义上有用的序列（例如单词、词干或词对），或许还有办法解决这个问题。然而，Transformer 内部生成的特征更像是输入对的集合，而非有序的输入集合。这意味着单词的表示将是一系列字符对，而这些字符对的顺序并没有得到很好的体现。Transformer 将被迫不断处理字谜，这大大增加了它的计算难度。事实上，对字符级表示的实验表明，Transformer 在处理这类表示时表现不佳。

### 字节对编码

幸运的是，这个问题有一个巧妙的解决方案，叫做[字节对编码](https://en.m.wikipedia.org/wiki/Byte_pair_encoding)。首先，在字符级表示中，每个字符都被分配一个代码，即一个唯一的字节。然后，在扫描一些代表性数据后，将最常见的字节对组合在一起，并分配一个新的字节，即一个新的代码。将这个新代码替换回数据中，并重复这个过程。

表示字符对的编码可以与其他字符或字符对的编码组合，从而得到表示更长字符序列的新编码。编码所能表示的字符序列长度没有限制。它们会根据需要不断增长，以表示常见的重复序列。字节对编码的巧妙之处在于，它能从数据中推断出需要学习的长字符序列，而不是简单地表示所有可能的序列。例如，它可以学习用单个字节编码来表示像*“transformer”*这样的长单词，但不会将编码浪费在长度相近的任意字符串上，例如*“ksowjmckder”*。而且，由于它保留了构成单个字符的所有字节编码，因此它仍然可以表示奇怪的拼写错误、新单词，甚至是外语。

使用字节对编码时，你可以为其指定词汇表大小，它会不断生成新的代码，直到达到该大小为止。词汇表必须足够大，才能使字符串足够长，从而捕捉文本的语义内容。它们必须有意义。这样，它们才能足够丰富，足以驱动变压器。

在训练或借用字节对编码器之后，我们可以用它来预处理数据，然后再将其输入到Transformer模型中。这会将连续的文本流分割成一系列不同的块（其中大部分应该是可识别的单词），并为每个块生成一个简洁的代码。这个过程称为分词。

### 音频输入

现在回想一下，我们最初的目标是将音频信号或语音指令转换为文本表示。到目前为止，我们所有的示例都是基于处理书面语言的字符和单词这一假设。我们也可以将其扩展到音频，但这需要对信号预处理进行更深入的探索。

音频信号中的信息需要经过一些复杂的预处理才能提取出来，从而帮助我们的耳朵和大脑理解语音。这种方法叫做梅尔频率倒谱滤波，正如其名，它确实非常复杂。如果您想深入了解其中的奥妙，这里有一篇图文并茂的[教程。](http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf)

预处理完成后，原始音频被转换成一系列向量，其中每个元素代表特定频率范围内音频活动的变化。该序列是稠密的（没有元素为零），且每个元素均为实数。

从积极的角度来看，每个向量对于Transformer来说都是一个很好的“词”或标记，因为它具有意义。它可以直接转换成一组可识别的、构成单词一部分的声音。

另一方面，将每个向量视为一个词的做法很奇怪，因为每个向量都是独一无二的。相同的向量值组合出现两次的可能性极低，因为声音之间存在着太多细微的差别。我们之前采用的独热编码和字节对编码策略都无济于事。

关键在于注意到，像这样的稠密实值向量正是*词嵌入后的结果*。Transformer 模型非常喜欢这种格式。为了利用它，我们可以像使用文本示例中嵌入的词一样，直接使用 Ceptrum 预处理的结果。这样就省去了分词和词嵌入的步骤。

值得注意的是，我们也可以用这种方法处理任何其他类型的数据。许多记录的数据都是以密集向量序列的形式呈现的。我们可以将它们直接输入到Transformer的编码器中，就像它们是嵌入词一样。

### 总结

如果你还在阅读，非常感谢。我希望这一切对你来说都是值得的。我们的旅程到此结束。我们最初的目标是为我们设想中的语音控制计算机制作一个语音转文本转换器。在这个过程中，我们从最基本的构建模块——计数和算术——入手，从零开始重建了一个转换器。我希望下次当你读到关于自然语言处理最新进展的文章时，能够心满意足地点头，因为你已经对背后的原理有了相当清晰的理解。

### 资源和版权

- 原版[论文](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)，《注意力就是你所需要的一切》。
- 这是一个非常有用的 Python[实现](https://nlp.seas.harvard.edu/2018/04/03/attention.html)的 transformer 函数。
- Jay Alammar 对Transformer的深入[讲解](https://jalammar.github.io/illustrated-transformer/)。
- 卢卡什·凯泽（作者之一）的[演讲，](https://www.youtube.com/watch?v%3DrBCqOTEfxvg)解释了变压器的工作原理。
- Google 幻灯片中的[插图](https://docs.google.com/presentation/d/1Po-GY7X-mXmPKHr8Vh29S4tFPv23TjeY-jq-yShlivM/edit?usp%3Dsharing)。

-------
[布兰登](https://brandonrohrer.com/blog.html)
更新于 2021年10月29日

[除另有说明外，所有文字和图片均采用 CC0 协议。](https://creativecommons.org/publicdomain/zero/1.0/)

本文观点仅代表我个人意见。

经认证，100% 人工生成